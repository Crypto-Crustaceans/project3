{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>updated_sentiment</th>\n",
       "      <th>sentiment_numeric</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Suggests BTC Price Floor Is $39K Survey ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-16T23:30:14+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microstrategy Buys 5050 More Bitcoins Now Hodl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-13T12:30:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salvadoran Governments Chivo Wallet Experience...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-09-08T15:30:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First Day of Bitcoin as Legal Tender: El Salva...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-07T21:30:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matrixport Launches BTC-U Range Sniper Returns...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-09-07T09:30:49+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence updated_sentiment  \\\n",
       "0  Model Suggests BTC Price Floor Is $39K Survey ...          positive   \n",
       "1  Microstrategy Buys 5050 More Bitcoins Now Hodl...          positive   \n",
       "2  Salvadoran Governments Chivo Wallet Experience...          negative   \n",
       "3  First Day of Bitcoin as Legal Tender: El Salva...          positive   \n",
       "4  Matrixport Launches BTC-U Range Sniper Returns...          positive   \n",
       "\n",
       "   sentiment_numeric                       date  \n",
       "0                  1  2021-09-16T23:30:14+00:00  \n",
       "1                  1  2021-09-13T12:30:07+00:00  \n",
       "2                 -1  2021-09-08T15:30:16+00:00  \n",
       "3                  1  2021-09-07T21:30:27+00:00  \n",
       "4                  1  2021-09-07T09:30:49+00:00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('news_sentiment.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "token_list = []\n",
    "for sentence in df['sentence']:\n",
    "    \n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        lower_list = []\n",
    "        for token in tokens:\n",
    "            result = ''.join(i for i in token if not i.isdigit())\n",
    "            result = PorterStemmer().stem(result)\n",
    "            if result.lower() == 'btc':\n",
    "                result = 'bitcoin'\n",
    "            if len(result) > 1:\n",
    "                lower_list.append(result.lower())\n",
    "            \n",
    "        token_list.append(lower_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_processed = []\n",
    "for token in token_list:\n",
    "    str1 = \" \"  \n",
    "    sentences_processed.append(str1.join(token))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences_processed\n",
    "y = df['updated_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, ngram_range=(1,2))\n",
    "\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "X_train_vector = vectorizer.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfid_vectorizer.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(vectorizer, 'tfid_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label-encode data\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear', C=1, gamma='auto')\n",
    "model.fit(X_train_vector, y_train_encoded)\n",
    "\n",
    "# param_grid = {'C': [1,5,10,25,50,100],\n",
    "#               'gamma': ['auto', .00001, .0001, .001, .005, .01, .05, .1, .5, 1]}\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# model = RandomForestClassifier(n_estimators=200)\n",
    "# model.fit(X_train_vector, y_train_encoded)\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# model = XGBClassifier(max_depth=8, n_estimators=500)\n",
    "# model.fit(X_train_vector, y_train_encoded)\n",
    "\n",
    "# param_grid = {'n_estimators': [100, 200, 500],\n",
    "#               'max_depth': [4, 8, 10]\n",
    "#               'gamma': np.arange(0:1,.01)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid = GridSearchCV(model, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.fit(X_train_vector, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List the best parameters for this dataset\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List the best score\n",
    "# print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6511156186612576"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_vector, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.45      0.54       221\n",
      "     neutral       0.64      0.84      0.73       502\n",
      "    positive       0.67      0.46      0.55       263\n",
      "\n",
      "    accuracy                           0.65       986\n",
      "   macro avg       0.66      0.58      0.60       986\n",
      "weighted avg       0.66      0.65      0.64       986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_encoded, predictions,\n",
    "                            target_names=[\"negative\", \"neutral\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headline_classifier.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "X_vector = vectorizer.transform(X)\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "model = SVC(kernel='linear', C=1, gamma='auto')\n",
    "model.fit(X_vector, y_encoded)\n",
    "\n",
    "dump(model, 'headline_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy.random import seed\n",
    "# seed(42)\n",
    "# from tensorflow import random\n",
    "# random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # one-hot encode\n",
    "\n",
    "# y_test_onehot = to_categorical(y_test_encoded)\n",
    "# y_train_onehot = to_categorical(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# deep_model = Sequential()\n",
    "# deep_model.add(Dense(units=6, activation='relu', input_dim=2))\n",
    "# deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "# deep_model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # X_test_vector = tf.keras.layers.TextVectorization(X_test, max_tokens=None, output_mode='int')\n",
    "# # X_train_vector = tf.keras.layers.TextVectorization(X_train)\n",
    "\n",
    "# vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "#  max_tokens=None,\n",
    "#  output_mode='tf_idf')\n",
    "\n",
    "# # Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# # dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# # datasets this means we're not keeping spare copies of the dataset.\n",
    "# vectorize_layer.adapt(X)\n",
    "\n",
    "# # Create the model that uses the vectorize text layer\n",
    "# deep_model = tf.keras.models.Sequential()\n",
    "\n",
    "# # Start by creating an explicit input layer. It needs to have a shape of\n",
    "# # (1,) (because we need to guarantee that there is exactly one string\n",
    "# # input per batch), and the dtype needs to be 'string'.\n",
    "# deep_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# # The first layer in our model is the vectorization layer. After this\n",
    "# # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# # indices.\n",
    "# deep_model.add(vectorize_layer)\n",
    "\n",
    "# # Now, the model can map strings to integers, and you can add an embedding\n",
    "# # layer to map these integers to learned embeddings.\n",
    "# # input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
    "# # model.predict(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the model to the training data\n",
    "# deep_model.fit(\n",
    "#     X_train.to_numpy(),\n",
    "#     y_train_onehot,\n",
    "#     epochs=100,\n",
    "#     shuffle=True,\n",
    "#     verbose=2\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
